{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw2_Part3_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azizamirsaidova/rossman_forecasting_sales/blob/main/Hw2_Part3_distribute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 3 : Extracting embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlux5Gn-n-Gz"
      },
      "source": [
        "We will repeat the first initial steps again from Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "f = ___________\n",
        "(X, y) = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following: \n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to **True** for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to **true** if you want to entity embeddings\n",
        "4. if 3 is set to **true**, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCk7tyVQEdy6"
      },
      "source": [
        "Split the data based on the train_size - same as part 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "  #your code here\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "X_val, y_val = sample(X_val, y_val, 200000)  # Simulate data sparsity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models with Entity embedding!!\n",
        "\n",
        "To read about Entity Embeddings in more detail and its use cases look at the follwing links:\n",
        "1. [Enhancing categorical features with Entity Embeddings](https://towardsdatascience.com/enhancing-categorical-features-with-entity-embeddings-e6850a5e34ff)\n",
        "2. [Understanding Entity Embeddings and Itâ€™s Application](https://towardsdatascience.com/understanding-entity-embeddings-and-its-application-69e37ae1501d#:~:text=Loosely%20speaking%2C%20entity%20embedding%20is,a%20sentence%2C%20or%20a%20paragraph.)\n",
        "\n",
        "The basic outline of this model will be something like this:\n",
        "\n",
        "\n",
        "![Outline.jpeg](https://drive.google.com/uc?export=view&id=1cm-jylknGEFg9KjehhRy90INkUJqRwrL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uOEqzd8BzK4"
      },
      "source": [
        "We will be creating the following embeddings:\n",
        "\n",
        "1. input_store - with input shape as (1,) \n",
        "  * The input_store will be passed to output_store to create an embedding layer with embedding shape as 1115, 10\n",
        "  * we will reshape this to target_shape=(10,)\n",
        "2. input_dow - with input shape as (1,) \n",
        "  * The input_dow will be passed to output_dow to create an embedding layer with embedding shape as 7, 6\n",
        "  * we will reshape this to target_shape=(6,)\n",
        "3. input_promo - with input shape as (1,) \n",
        "  * output promo will be a dense(1) for input_promo\n",
        "4. input_year - with input shape as (1,) \n",
        "  * The input_year will be passed to output_year to create an embedding layer with embedding shape as 3, 2\n",
        "  * we will reshape this to target_shape=(2,)\n",
        "5. input_month - with input shape as (1,) \n",
        "  * The input_month will be passed to output_month to create an embedding layer with embedding shape as 12, 6\n",
        "  * we will reshape this to target_shape=(6,)\n",
        "6. input_day - with input shape as (1,) \n",
        "  * The input_day will be passed to output_day to create an embedding layer with embedding shape as 31, 10\n",
        "  * we will reshape this to target_shape=(10,)\n",
        "7. input_germanstate - with input shape as (1,) \n",
        "  * The input_germanstate will be passed to output_germanstate to create an embedding layer with embedding shape as 12, 6\n",
        "  * we will reshape this to target_shape=(6,)\n",
        "\n",
        "\n",
        "* Construct an input model with all the inputs \n",
        "* Construct an output embeddings with all the outputs( basically the embeddings) - concatenate this and call it the **output model**\n",
        "\n",
        "Set the output model with the following parameters:\n",
        "\n",
        "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
        "4. Create a KerasModel called modelNN_emb:\n",
        "\n",
        "       modelNN_emb = KerasModel(inputs=input_model, outputs=output_model)\n",
        "4. Compile the model on mean absolute error and optimizer as adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGSxshWqRCu"
      },
      "source": [
        "#Define the embedding NN model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqUNSxR8q3-B"
      },
      "source": [
        "#we will rescale our y_train for the model\n",
        "#the reason for this is mentioned in the paper in the same section \n",
        "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val)))\n",
        "fitting_ytr = np.log(y_train) / max_log_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np80imB4MgYY"
      },
      "source": [
        "#do the same for y_val as well\n",
        "fitting_yval = np.log(y_val) / max_log_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WcPVMLiF-1v"
      },
      "source": [
        "Now that we have built the model, we need to ensure that the input passed to the model is also in the same format - hence we will define a function split the features we are performing embeddings on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZSq3RetpUx"
      },
      "source": [
        "#split the features\n",
        "def split_features(X):\n",
        "    X_list = []\n",
        "\n",
        "    store_index = X[..., [1]]\n",
        "    X_list.append(store_index)\n",
        "\n",
        "    day_of_week = X[..., [2]]\n",
        "    X_list.append(day_of_week)\n",
        "\n",
        "    promo = X[..., [3]]\n",
        "    X_list.append(promo)\n",
        "\n",
        "    year = X[..., [4]]\n",
        "    X_list.append(year)\n",
        "\n",
        "    month = X[..., [5]]\n",
        "    X_list.append(month)\n",
        "\n",
        "    day = X[..., [6]]\n",
        "    X_list.append(day)\n",
        "\n",
        "    State = X[..., [7]]\n",
        "    X_list.append(State)\n",
        "\n",
        "    return X_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn0ZRL6at7xg"
      },
      "source": [
        "def preprocessing(X):\n",
        "    X_list = split_features(X)\n",
        "    return X_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKlX4TdGNcn"
      },
      "source": [
        "1. Add a ModelCheckpoint to save the weights \n",
        "\n",
        "2. Fit the model on the `preprocessing(X_train)` and on the fitting_y with 10 epochs and batch_size as 128 \n",
        "  * add in callback here for ModelCheckPoint and also provide validation data here. \n",
        "  * ModelCheckPoint will be used to save the weights for the model, we will use these weights for each embedding layer( you can read more about model check point [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCE5qF0lrEmJ"
      },
      "source": [
        "_______.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
        "_______.fit(preprocessing(X_train), fitting_ytr, epochs = 10, batch_size = 128, validation_data= (preprocessing(X_train),fitting_yval), callbacks=[modelNN_emb.checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX4q6sj2qBTy"
      },
      "source": [
        "Now we will save the embeddings from the model defined above:\n",
        "check if save_embeddings is set to True first, if yes store the following:\n",
        "  1. store_embedding\n",
        "  2. dow_embedding\n",
        "  3. year_embedding\n",
        "  4. month_embedding\n",
        "  5. day_embedding\n",
        "  6. state_embedding\n",
        "\n",
        "\n",
        "Save this entire embeddings into the pickle file - **saved_embeddings_fname - embeddings.pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7QS4kLLqKrH"
      },
      "source": [
        "if save_embeddings:\n",
        "    store_embedding = modelNN_emb.get_layer('store_embedding').get_weights()[0]\n",
        "    dow_embedding = modelNN_emb.get_layer('dow_embedding').get_weights()[0]\n",
        "    year_embedding = modelNN_emb.get_layer('year_embedding').get_weights()[0]\n",
        "    month_embedding = modelNN_emb.get_layer('month_embedding').get_weights()[0]\n",
        "    day_embedding = modelNN_emb.get_layer('day_embedding').get_weights()[0]\n",
        "    german_states_embedding = modelNN_emb.get_layer('state_embedding').get_weights()[0]\n",
        "    with open(saved_embeddings_fname, 'wb') as f:\n",
        "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
        "                     month_embedding, day_embedding, german_states_embedding], f, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5mmmJi_BCc"
      },
      "source": [
        "# You are done with Part 3!! \n",
        "Remember to save the embeddings.pickle - this will be used in Part 4!"
      ]
    }
  ]
}