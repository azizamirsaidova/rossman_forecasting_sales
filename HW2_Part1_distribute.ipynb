{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_Part1_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 1: Preprocessing\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRlyqZTz4CJ"
      },
      "source": [
        "#### **Name of people who have worked on this homework:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQSvK4tZ3ias"
      },
      "source": [
        "## Table of Contents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llCqCNUTz7wG"
      },
      "source": [
        "* **HW-2: Rossman Kaggle: Forecasting Sales**\n",
        "  * Instructions\n",
        "  * Learning Goals\n",
        "  * Loading the DataFrame\n",
        "  * Q1: Data-Preprocesing and Understanding the data **(10 marks)**(HW1_Part1)\n",
        "  * Q2: Modelling without Entity Embeddings**(30 marks)**(HW1_Part2) \n",
        "    * 2.1 Random Forest \n",
        "    * 2.2 XGBoost \n",
        "    * 2.3 Multi Layer Perceptron \n",
        "  * Q3: Modelling MLP with Entity Embeddings**(10 marks)**(HW1_Part3)\n",
        "  * Q4 : Modelling other models with Entity Embeddings **(40 marks)**(HW1_Part4)\n",
        "    * 4.1 Random Forest \n",
        "    * 4.2 XGBoost\n",
        "  * Q4: Final Comments **(10 marks)** (HW1_Part4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxkV2910pFEw"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhwik43rpHj_"
      },
      "source": [
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To work on the homework, you will first need to fork the repository into your GitHub account and clone it to work on it on your local computer. To submit your homework, push your homework into the same GitHub and upload the link on edStem.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0xOsPAh0NI7"
      },
      "source": [
        "## Learning Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTWGfd3i0QI6"
      },
      "source": [
        "**We will look here into the practicalities of Trees, MLPs and Entity Embedding.**\n",
        "\n",
        "The homework is divided into four main parts:\n",
        "1. Data-preprocessing\n",
        "2. Developing different models and evaulating the models - without Entity Embeddings\n",
        "3. Pass on the entity embeddings from Neural Network model to other models and evaluate the models\n",
        "4. Compare the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sed_rlx_pU3s"
      },
      "source": [
        "## Read this first!\n",
        "\n",
        "The homework is divided into **4 notebooks**\n",
        "1. Preprocessing and Storing Data\n",
        "2. Modelling without Entity Embeddings\n",
        "3. MultiLayer Perceptron with Entity Embeddings \n",
        "4. Modelling with Entity Embeddings and Comparing the results\n",
        "\n",
        "\n",
        "This Homework is based on the **paper attached in the data folder**\n",
        "\n",
        "Lets talk about the paper first:\n",
        "\n",
        "A very simple explaination of what the paper is trying to achieve is to show how to accuracy of the model changes using Entity Embeddings. You will first pre-process the data, pass it through tree models and MLP and check the MAPE. After that you will build another MLP Model with embeddings. This embedding features will be extracted and then merged with the train set and then passed as input to the same tree models to check their MAPE. \n",
        "\n",
        "**Things to note:**\n",
        "\n",
        "1. We want the results to be **almost same** as the results shown in the paper(Your results will not be exactly the same):\n",
        "\n",
        "![Results.jpeg](https://drive.google.com/uc?export=view&id=1KqzimhXso6aojPYwcBNj5EnDNZoY_Hqb)\n",
        "\n",
        "\n",
        "**We will not be implementing KNNs**\n",
        "\n",
        "2. The paper specifically mentions the parameters it uses to achieve these results, and we will be using the **same as well**. \n",
        "![Parameters.jpeg](https://drive.google.com/uc?export=view&id=1ROfqM3F5hWwJyrvQr_J1ATovNIW5niOs)\n",
        "\n",
        "**Again remember we will not be implementing KNNs**\n",
        "\n",
        "\n",
        "3. The last point we want you to note is the following: we will be using MAPE\n",
        "\n",
        "\n",
        "![Mape.jpeg](https://drive.google.com/uc?export=view&id=1UFi9yWzmSWePNm2qpRqGKylR5Q5-Q7ms)\n",
        "\n",
        "\n",
        "4. Read the paper first!! Specifically **B. Comparison of different methods** **This will give you more clarity on what goes as an input to each model, hence your results will be as close as possible to the paper.** \n",
        "\n",
        "#### So lets get started! Please note: this particular notebook is only for Data Preprocessing and saving the datafile. The notebooks for Modelling without Entity Embeddings MLP with Entity Embedding and other models with Entity Embeddings is Part2 and Part3 and Part4. \n",
        "\n",
        "**Why are we doing this?** \n",
        "\n",
        "Each of this processing requires high RAM, which you may or may not have access to - hence we split the work in four parts and call the work from each part into the next one! Also this helps us modularise it better!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ch786JHpRvl"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRf1faokxPi"
      },
      "source": [
        "## Q1. Data Pre-Processing and Saving the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoDLSZOkxPi"
      },
      "source": [
        "### 1.1 Loading and understanding the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdAE5YJCkxPj"
      },
      "source": [
        "#### About the data\n",
        "\n",
        "Most of the fields are self-explanatory. The following are descriptions for those that aren't. \n",
        "\n",
        "1. **Id** - an Id that represents a (Store, Date) duple within the test set\n",
        "2. **Store** - a unique Id for each store\n",
        "3. **Sales** - the turnover for any given day (this is what you are predicting)\n",
        "4. **Customers** - the number of customers on a given day\n",
        "5. **Open** - an indicator for whether the store was open: \n",
        "    * 0 = closed\n",
        "    * 1 = open\n",
        "6. **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. \n",
        "    * a = public holiday \n",
        "    * b = Easter holiday\n",
        "    * c = Christmas\n",
        "    * 0 = None\n",
        "7. **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "8. **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "9. **Assortment** - describes an assortment level: \n",
        "    * a = basic\n",
        "    * b = extra\n",
        "    * c = extended\n",
        "10. **CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "11. **CompetitionOpenSince[Month/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n",
        "12. **Promo** - indicates whether a store is running a promo on that day\n",
        "13. **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: \n",
        "    * 0 = store is not participating\n",
        "    * 1 = store is participating\n",
        "14. **Promo2Since[Year/Week]** - describes the year and calendar week when the store started participating in Promo2\n",
        "15. **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81YPHFtmkxPj"
      },
      "source": [
        "**Note, since this data is large, we do not want to convert this data into dataframes, we will store it as array of dictionaries and pass the same to the models.**\n",
        "**Also, we reccommend using Google Colab for completing this Homework.** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWNvvXvduWrV"
      },
      "source": [
        "#importing the data as a string \n",
        "#your code here \n",
        "train_set = (\"________\") #train.csv in rossmann kaggle folder(zip file)\n",
        "stores_set = (\"________\") #stores.csv in rossmann kaggle folder(zip file)\n",
        "store_states = (\"________\") #store_states.csv in data folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtgypPZUDZdI"
      },
      "source": [
        "We will now define functions:\n",
        "1. To convert our csv files into dictionaries\n",
        "2. To replace nan values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9udTMgNsxk7g"
      },
      "source": [
        "def csv2dicts(csvfile):\n",
        "    data = []\n",
        "    keys = []\n",
        "    for row_index, row in enumerate(csvfile):\n",
        "        if row_index == 0:\n",
        "            keys = row\n",
        "            print(row)\n",
        "            continue\n",
        "        data.append({key: value for key, value in zip(keys, row)})\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FBsI4Zxm8_"
      },
      "source": [
        "def set_nan_as_string(data, replace_str='0'):\n",
        "    for i, x in enumerate(data):\n",
        "        for key, value in x.items():\n",
        "            if value == '':\n",
        "                x[key] = replace_str\n",
        "        data[i] = x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txMeck53u6Rf"
      },
      "source": [
        "Save the train_set as a dictionary using csv2dicts function defined above. \n",
        "\n",
        "Further save this as a pickle file - call it **train_set.pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhynS2EgvDtw"
      },
      "source": [
        "# save the train_set as a dictionary using csv2dicts function defined above. \n",
        "# Save this as a pickle file - call it train_set.pickle\n",
        "#your code here\n",
        "with open(train_set) as csvfile:\n",
        "    data = csv.reader(csvfile, delimiter=',')\n",
        "    with open('train_set.pickle', 'wb') as f:\n",
        "        data = csv2dicts(data)\n",
        "        data = data[::-1]\n",
        "        pickle.dump(data, f, -1)\n",
        "        print(data[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Byj46QzvfwB"
      },
      "source": [
        "If you look at store_states - it is basically sharing information about which stores are located in which states. Hence we will add this in the stores_set itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVHbmWIVvKOY"
      },
      "source": [
        "#lets do the same thing what we did above for the store_set and store_states - call this pickle as store_set.pickle\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqZtSnaRv86g"
      },
      "source": [
        "Next we want to store the train_data length, hence load the data back from the pickle files saved and only assign num_records as the length of the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X2PVJRivi0Y"
      },
      "source": [
        "with open('train_set.pickle', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    num_records = len(train_data)\n",
        "with open('store_set.pickle', 'rb') as f:\n",
        "    store_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WuDe_H9wImc"
      },
      "source": [
        "If you have saved and loaded the files correctly then **train_data[1]** and **store_data[1]** should be as follows:\n",
        "\n",
        "![Mape.jpeg](https://drive.google.com/uc?export=view&id=1D7IMgfjbRvWNuJV_v5nx5H7TfzGjP811)\n",
        "\n",
        "\n",
        "Check if the column names are the same - if not recheck the previous codes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK8-EY45v-eI"
      },
      "source": [
        "#check the same\n",
        "train_data[1], store_data[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYxIcQVHkW5Z"
      },
      "source": [
        "### 1.2 Feature list\n",
        "\n",
        "We will define a function to extract features from the data - this will be the final data passed to all the models. \n",
        "Why are we doing this? - we dont need all the features from the train_set or stores_set to predict sales . we will pick a few selected ones  only.\n",
        "\n",
        "The function should return the following paramters:\n",
        "* the **store index** = from the train_set it should show the 'store'\n",
        "* **year** = this should come from train_set 'Date'\n",
        "* **month** = this should come from train_set 'Date'\n",
        "* **day** = this should come from train_set 'Date'\n",
        "* **day_of_week** = this should come from train_set 'DayOfWeek'\n",
        "* check if the **store is open** \n",
        "    * if yes - save that \n",
        "    * else it should save 1\n",
        "* **promo** = this should come from train_set 'Promo'\n",
        "* **store_state** = this should come from store_state 'State'\n",
        "\n",
        "\n",
        "Note the year month and day will come from Date - this is a string and has to be split for each individual values, you might want to use **[datetime.strptime](https://www.programiz.com/python-programming/datetime/strptime)** for this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V94HyNa9YZpv"
      },
      "source": [
        "def feature_list(record):\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_xfJP_UlI5x"
      },
      "source": [
        "Now lets create two dictionaries - train_data_X and train_data_y \n",
        "\n",
        "* Run through the train_set, and check if the 'Sales' are not equal to 0 and 'Open' is not equal to 0 ( we do not want to store features for which sales is zero and the store is not open)\n",
        "* If yes(store is open and sales is not 0), then store the features(from **feature list function**) into a variable named f1\n",
        "* append the f1 values in train_data_X\n",
        "* append the **Sales not equal** to 0 to train_data_y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi0VVRT2Yon7"
      },
      "source": [
        "train_data_X = []\n",
        "train_data_y = []\n",
        "\n",
        "for record in train_data:\n",
        "  #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ0pwPL5xktL"
      },
      "source": [
        "#again check how your train_data_X looks\n",
        "train_data_X[1]\n",
        "\n",
        "# at this point your data should have 8 values - something like this [1, 948, 2, 0, 2013, 1, 1, 'BW']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMq8Ebf2o8Vz"
      },
      "source": [
        "The next step is going to be labelencoding(the idea is to actually ordinally encode it, but we can use LabelEncoding here as well - look at the sklearn documentation for both) the train_data_X. We do this using LabelEncoder from sklearn\n",
        "\n",
        "We will run this for the **complete train_data_X**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lapEjeLamUvd"
      },
      "source": [
        "check_X = train_data_X\n",
        "check_X = np.array(check_X)\n",
        "train_data_X = np.array(train_data_X)\n",
        "les = []\n",
        "for i in range(train_data_X.shape[1]):\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaAS0GmYx7Je"
      },
      "source": [
        "#again check how your train_data_X looks \n",
        "train_data_X[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Ine8QPyWYL"
      },
      "source": [
        "We will dump the les dictionary(defined in the previous step) into les.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znP43J2PJjbO"
      },
      "source": [
        "And convert our train_data_X as **int** datatype, and save our train_data_y as an **numpy array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIFRlmcszPMu"
      },
      "source": [
        "with open('les.pickle', 'wb') as f:\n",
        "    pickle.dump(les, f, -1)\n",
        "train_data_X = train_data_X.astype(int)\n",
        "train_data_y = np.array(train_data_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOx4-rgAy-U_"
      },
      "source": [
        "Finally we will store our train_data_X, train_data_y in a pickle file - **feature_train_data.pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVr3ivy2zQ_v"
      },
      "source": [
        "with open('feature_train_data.pickle', 'wb') as f:\n",
        "    pickle.dump((train_data_X, train_data_y), f, -1)\n",
        "    print(train_data_X[0], train_data_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0ygwmq2agg"
      },
      "source": [
        "## You are done with Part 1 of the Homework!\n",
        "\n",
        "\n",
        "Save all the pickle files locally in your system/drive - these will be used in the next parts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ngVK2bfOW_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}