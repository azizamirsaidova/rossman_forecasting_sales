{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_Part2_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azizamirsaidova/rossman_forecasting_sales/blob/main/HW2_Part2_distribute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 2 : Modelling without embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sEs45v7Rr2"
      },
      "source": [
        "## Part 2: Modelling without Entity Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7vU2zkGSXG"
      },
      "source": [
        "Remember the parameters we need to use\n",
        "\n",
        "![Parameters.jpeg](https://drive.google.com/uc?export=view&id=1ROfqM3F5hWwJyrvQr_J1ATovNIW5niOs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "f = __________\n",
        "(X, y) = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following:\n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
        "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "one_hot_as_input = True #one_hot is set to True\n",
        "embeddings_as_input = False #embeddings later on needs to set to true for Part 3\n",
        "save_embeddings = False\n",
        "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSoah0_F9Ed"
      },
      "source": [
        "Define a function to one hot encode the training set and after split transform your training set using the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS03CrxWjcNN"
      },
      "source": [
        "def ________:\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCk7tyVQEdy6"
      },
      "source": [
        "Split the data into X_train, X_val, y_train, y_val based on the train_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1-ykazxEhLC"
      },
      "source": [
        "Lets also sample the data\n",
        "\n",
        "**Why do we do this??**\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models without embedding!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5efxrLE7Cd"
      },
      "source": [
        "**Lets define MAPE first**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRpKGtOLE6TV"
      },
      "source": [
        "#defining mape\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    #your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1c33g9Kjlcr"
      },
      "source": [
        "**We will be log-transforming the dependent variable(y) because it is long-tailed** - keep this in mind for each model or do the conversion after you split the data itself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxpJyr-Gct-"
      },
      "source": [
        "### 2.1: Random Forests\n",
        "\n",
        "1. Define a RandomForestRegressor model - with n_esitmators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1\n",
        "2. Fit on the training data\n",
        "3. Predict on the validation and training data\n",
        "4. evaluate the model - calculate the MAPE for validation and training data\n",
        "\n",
        "**These parameters are from the paper** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGX-_sRYkxPv"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSEUp3GHC_Q"
      },
      "source": [
        "### 2.2 Boosted Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cq3sVKaHRAL"
      },
      "source": [
        "For boosting, we will use XGBoost for regression\n",
        "1. We will create a DMatrix from XGB for this - because we want to define a param_grid here. \n",
        "  * Again look at the parameters from the paper\n",
        "2. The DMatrix should be provided with X_train and label as y_train\n",
        "3. Parameters will be as follows:\n",
        "  * 'nthread': -1,\n",
        "  * 'max_depth': 7,\n",
        "  * 'eta': 0.02,\n",
        "  * 'silent': 1,\n",
        "  * 'objective': 'reg:linear',\n",
        "  * 'colsample_bytree': 0.7,\n",
        "  * 'subsample': 0.7\n",
        "  * num_round = 3000\n",
        "\n",
        "4. Train the model\n",
        "\n",
        "5. Note xgb.DMatrix needs features from Xtrain and Xval while predicting. Hence define:\n",
        "```\n",
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "```\n",
        "5. Predict on feature_Xtr and feature_Xval \n",
        "6. Calculate MAPE for both\n",
        "\n",
        "\n",
        "\n",
        "Look at XGBoost [documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) for each parameter information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1--s_4pGOuB"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LIINAciJtWi"
      },
      "source": [
        "### 2.3 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wdjE8AgN-Ma"
      },
      "source": [
        "Define a Sequential model with the following:\n",
        "(Read the Part VI Part A Neural Networks)\n",
        "\n",
        "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
        "4. Compile the model on mean absolute error and optimizer as adam\n",
        "5. Fit the model on 10 epochs and batch size as 128, find the MAPE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_wvmj1lNmhR"
      },
      "source": [
        "#Build the model\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qcYmI5la84"
      },
      "source": [
        "#we will rescale our y_train for the model\n",
        "#the reason for this is mentioned in the paper in the same section\n",
        "# to see this change you can plot the log(Sale) vs log(Sale_max) and see how it varies\n",
        "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val)))\n",
        "fitting_y = np.log(y_train) / max_log_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD14nr4ik0re"
      },
      "source": [
        "#fit your model \n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN0zoTVdG2MI"
      },
      "source": [
        "#predict and mape calculation\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-5celHwnaLO"
      },
      "source": [
        "# You are done with Part 2!!\n",
        "Print out the MAPE values for all models, you will need this in hand while working on Part 3 for comparing!"
      ]
    }
  ]
}